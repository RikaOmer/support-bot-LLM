import os
import sys
from langchain_chroma import Chroma
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_huggingface import HuggingFaceEmbeddings

# --- ×”×’×“×¨×•×ª ---
DB_PATH = "chroma_db"
# ××•×“×œ×™× ××©×•×¤×¨×™× - ×¢×“×™×™×Ÿ ×§×œ×™× ×œ××—×©×‘:
# ××¤×©×¨×•×™×•×ª LLM (Ollama): "llama3.1" (××•×ª×§×Ÿ), "mistral:7b", "phi3:mini", "llama3.2:3b"
OLLAMA_MODEL = "llama3.1"  # ××•×ª×§×Ÿ ×‘××—×©×‘ ×©×œ×š, ××™×›×•×ª ××¢×•×œ×”
# ××¤×©×¨×•×™×•×ª Embedding: "intfloat/multilingual-e5-small" (×§×œ ×™×•×ª×¨), "paraphrase-multilingual-mpnet-base-v2" (××™×›×•×ª ×˜×•×‘×” ×™×•×ª×¨)
HF_MODEL_NAME = "intfloat/multilingual-e5-small"  # ×§×œ ×××•×“ (~130MB), ××¦×•×™×Ÿ ×œ×¢×‘×¨×™×ª

def format_docs(docs):
    """××¢×¦×‘ ××ª ×”××¡××›×™× ×©× ××¦××• ×œ×¤×•×¨××˜ ×‘×¨×•×¨ ×¢× ×”×¤×¨×“×”"""
    if not docs:
        return "×œ× × ××¦× ××™×“×¢ ×¨×œ×•×•× ×˜×™ ×‘×××’×¨ ×”× ×ª×•× ×™×."
    
    formatted_parts = []
    for i, doc in enumerate(docs, 1):
        # ×”×•×¡×¤×ª ××™×“×¢ ×¢×œ ×”××§×•×¨
        source = doc.metadata.get('source', '×œ× ×™×“×•×¢')
        header_info = doc.metadata.get('Header 1', '')
        if header_info:
            source += f" > {header_info}"
        
        # ×”×•×¡×¤×ª ×”×ª×•×›×Ÿ ×¢× ×”×¤×¨×“×” ×‘×¨×•×¨×”
        formatted_parts.append(
            f"[××§×•×¨ {i}: {source}]\n{doc.page_content}\n"
        )
    
    return "\n---\n".join(formatted_parts)

def start_chat():
    print("ğŸ“‚ ×˜×•×¢×Ÿ ××ª ×”×“××˜×”-×‘×™×™×¡...")
    print(f"ğŸ”¤ ××•×“×œ Embedding: {HF_MODEL_NAME}")
    print(f"ğŸ¤– ××•×“×œ LLM: {OLLAMA_MODEL}")
    embedding_function = HuggingFaceEmbeddings(model_name=HF_MODEL_NAME)
    
    if not os.path.exists(DB_PATH):
        print("âŒ ×©×’×™××”: ×”×“××˜×” ×‘×™×™×¡ ×œ× × ××¦×.")
        return

    vector_store = Chroma(persist_directory=DB_PATH, embedding_function=embedding_function)
    
    # ×”×’×“×œ× ×• ××ª k ×œ-6 ×›×“×™ ×œ×ª×¤×•×¡ ×™×•×ª×¨ ×”×§×©×¨
    # ×‘××§×•× similarity, × ×¡×” MMR ×œ×—×™×¤×•×© ××’×•×•×Ÿ ×™×•×ª×¨
    retriever = vector_store.as_retriever(
        search_type="mmr",
        search_kwargs={"k": 6, "fetch_k": 12}  # ×‘×•×“×§ 12, ××—×–×™×¨ 6 ×”×˜×•×‘×™× ×‘×™×•×ª×¨
    )
    llm = ChatOllama(model=OLLAMA_MODEL, temperature=0)

    template = """××ª×” ×¢×•×–×¨ ×˜×›× ×™ ××§×¦×•×¢×™.

×”×•×¨××•×ª ×—×©×•×‘×•×ª:
1. ×”×©×ª××© ×¨×§ ×‘××™×“×¢ ×©××•×¤×™×¢ ×‘-Context ×œ××˜×”. ××œ ×ª××¦×™× ×ª×©×•×‘×•×ª.
2. ×× ×”×ª×©×•×‘×” ×œ× × ××¦××ª ×‘-Context, ×›×ª×•×‘ ×‘×‘×™×¨×•×¨: "×”××™×“×¢ ×”××‘×•×§×© ×œ× × ××¦× ×‘×××’×¨ ×”× ×ª×•× ×™× ×©×œ×™."
3. ×›×©××—×¤×©×™× ×§×•×“, ××¡×¤×¨ ×˜×œ×¤×•×Ÿ, ××• ××™×“×¢ ×¡×¤×¦×™×¤×™ - ×‘×“×•×§ ××ª ×”×˜×‘×œ××•×ª ×•×”×¨×©×™××•×ª ×‘-Context ×‘×§×¤×™×“×”.
4. ×× ×™×© ×§×™×©×•×¨ ×œ×ª××•× ×” (×›××• `![alt](path)`), ×›×œ×•×œ ××•×ª×• ×‘×¡×•×£ ×”×ª×©×•×‘×”.
5. ×ª××™×“ ×¢× ×” ×‘×¢×‘×¨×™×ª.
6. ×× ×™×© ××¡×¤×¨ ××§×•×¨×•×ª ×¨×œ×•×•× ×˜×™×™×, ×¦×™×™×Ÿ ××ª ×”××§×•×¨ ×©×œ ×›×œ ×—×œ×§ ×‘×ª×©×•×‘×”.

Context:
{context}

×©××œ×”:
{question}

×ª×©×•×‘×”:"""

    prompt = ChatPromptTemplate.from_template(template)
    
    # ×©×¨×©×¨×ª RAG ××©×•×¤×¨×ª ×¢× ×¢×™×¦×•×‘ × ×›×•×Ÿ ×©×œ ×”××¡××›×™×
    rag_chain = (
        {
            "context": retriever | format_docs,  # ×¢×™×¦×•×‘ ×”××¡××›×™× ×œ×¤× ×™ ×©×œ×™×—×” ×œ-LLM
            "question": RunnablePassthrough()
        }
        | prompt
        | llm
        | StrOutputParser()
    )

    print("\nğŸ¤– ×”×‘×•×˜ ××•×›×Ÿ! (×›×ª×•×‘ 'exit' ×œ×™×¦×™××”)\n")
    
    while True:
        query = input("×©××œ ××•×ª×™: ")
        if query.lower() in ['exit', 'quit', '×™×¦×™××”']:
            break
            
        print("\nğŸ” ××—×¤×© (DEBUG MODE)...")
        docs = retriever.invoke(query)
        
        # --- ×”×“×¤×¡×ª ×“×™×‘××’: ××” ×”×‘×•×˜ ×‘×××ª ×¨×•××”? ---
        if docs:
            print(f"âœ… × ××¦××• {len(docs)} ××§×•×¨×•×ª.")
            for i, doc in enumerate(docs[:3], 1):  # ××¦×™×’ ×¨×§ 3 ×¨××©×•× ×™×
                source = doc.metadata.get('source', '×œ× ×™×“×•×¢')
                header = doc.metadata.get('Header 1', '')
                preview = doc.page_content[:100].replace('\n', ' ')
                print(f"   [{i}] {source}" + (f" > {header}" if header else ""))
                print(f"       {preview}...")
            print()
        else:
            print("âš ï¸ ×œ× × ××¦××• ××¡××›×™×.\n")

        print("ğŸ’¡ ×ª×©×•×‘×”:")
        for chunk in rag_chain.stream(query):
            print(chunk, end="", flush=True)
        print("\n" + "-"*50 + "\n")

if __name__ == "__main__":
    start_chat()
